# 2022
* **Robotics Transformer 1** (**RT-1**)
  * title and link: [RT-1: Robotics Transformer for Real-World Control at Scale](https://arxiv.org/abs/2212.06817)
  * information: 2022.12.16 Arxiv Google (Chelsea Finn, Sergey Levine)
  * problem and position: first attempt to brute-force general robot manipulation
  * method overview: large real-world data and Transformer-based model
  * results: 
    ![RT-1_result1](assets/2022/RT-1_result1.png)
    ![RT-1_result2](assets/2022/RT-1_result2.png)
  * method details: 
    * training dataset with ~130k episodes over 700+ tasks, collected for 17 months with 13 robots
    * open-source
    ![RT-1_method1](assets/2022/RT-1_method1.png)
    * input 6 images and language instructions and output robot actions
    * image tokenized by ImageNet-pretrained EfficientNet-B3 into $9 \times 9 \times 512 \rightarrow 81 \times 512$
    * instruction tokenized by Universal Sentence Encoder into $512$ embedding
    * FiLM layers condition instruction embedding to image tokenizer
    * TokenLearner as element-wise attention map 81 to 8 tokens to speed up inference
    * decoder-only Transformer
    * 35M parameters, 3Hz, open-source
    ![RT-1_method2](assets/2022/RT-1_method2.png)
    ![RT-1_method3](assets/2022/RT-1_method3.png)

* **do as i Can not as i Say** (**SayCan**)
  * information: 2022.08.16 CoRL 2022 special innovation paper Google (Chelsea Finn, Sergey Levine)
  * see [honors](https://github.com/dadadadawjb/honors)
