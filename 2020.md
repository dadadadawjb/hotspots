# 2020
* **Point Transformer V1** (**PTv1**)
  * title and link: [Point Transformer](https://arxiv.org/abs/2012.09164)
  * information: 2020.12.16 ICCV 2021 oral HKU
  * problem and position: first attempt for Transformer backbone on point clouds
  * method overview: just self-attention layers on point tokens
  * results: 
    ![PTv1_result1](assets/2020/PTv1_result1.png)![PTv1_result2](assets/2020/PTv1_result2.png)![PTv1_result3](assets/2020/PTv1_result3.png)
  * method details: 
    * each point as a token
    * vector self-attention within k nearest neighbors
      ![PTv1_method1](assets/2020/PTv1_method1.png)
    * positional encoding by MLP on point coordinates
      ![PTv1_method2](assets/2020/PTv1_method2.png)
    ![PTv1_method3](assets/2020/PTv1_method3.png)
    ![PTv1_method4](assets/2020/PTv1_method4.png)
